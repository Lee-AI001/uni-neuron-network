
Conculsion from data: 

1. Accuracy (Test MSE and R²)

Normal Network:

Best Performance (LR=0.025, Large Group): Test MSE 0.0057, R² 0.8530.
Worst Performance (LR=0.001–0.05, Long Group): Test MSE 0.0399, R² ≈ -0.0264.
Observation: Normal networks struggle significantly with deeper structures (Long group), failing to converge at all learning rates (R² near 0 or negative), indicating poor learning capacity for complex architectures. Higher learning rates (0.025–0.05) improve performance in shallower/wider structures (Mid, Large), but accuracy remains limited compared to advanced variants.

Advanced-Normal Network:

Best Performance (LR=0.01, Large Group): Test MSE 0.0047, R² 0.8792.
Worst Performance (LR=0.001, Long Group): Test MSE 0.0178, R² 0.5416.
Observation: BatchNorm and Dropout enhance performance over Normal NN, with Test MSE ranging from 0.0047 to 0.0185 and R² from 0.5252 to 0.8792. However, deeper structures (Long) underperform at low learning rates (e.g., 0.001), suggesting that regularization alone isn’t sufficient without architectural support.

Uni-Neuron Network (Uni-NN):

Best Performance (LR=0.025, Small Group): Test MSE 0.0060, R² 0.8455.
Worst Performance (LR=0.001, Mid Group): Test MSE 0.0160, R² 0.5880.
Observation: Skip connections offer an edge over Normal networks, especially in smaller or deeper setups. The best result (Test MSE 0.0060, R² 0.8455) at LR=0.025 suggests Uni-NN benefits from higher learning rates, likely due to improved gradient flow, though it doesn’t consistently match advanced variants.

Advanced-Uni-Neuron Network (AdvUni-NN):

Best Performance (LR=0.05, Long Group): Test MSE 0.0044, R² 0.8859.
Worst Performance (LR=0.001, Small Group): Test MSE 0.0097, R² 0.7504.
Observation: AdvUni-NN performs reliably across all groups, with Test MSE from 0.0044 to 0.0097 and R² from 0.7504 to 0.8859. It shines in the Long group, where depth and skip connections, combined with BatchNorm and Dropout, yield the best accuracy (Test MSE 0.0044, R² 0.8859 at LR=0.05). It’s less dominant in smaller groups compared to Large or Long setups.

2. Compute Speed

Training Time:

Normal: 24–43s (fastest).
Advanced-Normal: 67–162s.
Uni-NN: 33–88s.
AdvUni-NN: 76–207s (slowest).
Observation: AdvUni-NN’s superior accuracy comes at the cost of significantly longer training times due to BatchNorm, Dropout, and skip connections. Normal NN is the fastest but least accurate, while Uni-NN offers a middle ground. Training time scales with network depth and complexity (e.g., Long AdvUni-NN: 207s vs. Small Normal: 24s).

3. Generalization (Overfit Gap)

Normal:
Positive Overfit Gap (0.0004–0.0040) across most LRs and sizes, indicating underfitting; Long group consistently poor (0.0040).

Advanced-Normal:
Small Overfit Gap (-0.0016 to 0.0026), showing better generalization with occasional slight overfitting at high LRs (e.g., -0.0016, Large, LR=0.05).

Uni-NN:
Positive Overfit Gap (0.0001–0.0039) at low LRs, near zero or negative (-0.0023 to 0.0014) at high LRs, suggesting improved generalization with learning rate increases.

AdvUni-NN:
Small Overfit Gap (-0.0027 to 0.0025), demonstrating good generalization across all LRs and sizes, with slight underfitting at high LRs (e.g., -0.0027, Large, LR=0.05).

Observation: AdvUni-NN generalizes decent, a bit behind Advanced neuron network. Uni-NN improves with higher LRs, while Normal NN struggles, especially in deeper structures.

4. Impact of Learning Rate

Low LRs (0.001–0.0025): Slow convergence, especially for Normal (Long Test MSE 0.0399) and Uni-NN (Mid Test MSE 0.0160), with AdvUni-NN still achieving decent results (Small Test MSE 0.0097).

Moderate LRs (0.005–0.0075): Significant improvement across all models, with AdvUni-NN excelling (e.g., Mid Test MSE 0.0067, R² 0.8265 at LR=0.0075).

High LRs (0.025–0.05): Best performance for AdvUni-NN (Long Test MSE 0.0044, R² 0.8859 at LR=0.05) and Uni-NN (Small Test MSE 0.0060 at LR=0.025), with Normal and Advanced-Normal showing slight overfitting (e.g., Large Normal gap -0.0022 at LR=0.05).

Observation: Higher learning rates (0.025–0.05) optimize AdvUni-NN and Uni-NN, likely due to skip connections mitigating gradient issues, while Normal NN benefits less.

5. Parameter Count vs. Performance

Small (165 params, AdvUni-NN): Test MSE 0.0047, R² 0.8794 (LR=0.05).
Mid (277 params, AdvUni-NN): Test MSE 0.0059, R² 0.8494 (LR=0.05).
Large (629 params, AdvUni-NN): Test MSE 0.0054, R² 0.8620 (LR=0.05).
Long (313 params, AdvUni-NN): Test MSE 0.0044, R² 0.8859 (LR=0.05).

Observation: AdvUni-NN performs best in Long (313 params) and Large (629 params) setups, where depth or width enhances accuracy. Smaller groups (Small, Mid) are less impressive, suggesting depth matters more than raw parameter count.

Conclusion

For the Concrete Compressive Strength regression task, the Advanced-Uni-Neuron Network (AdvUni-NN) stands out as a strong performer, particularly in the Long group, where it achieves the lowest Test MSE (0.0044) and highest R² (0.8859) at LR=0.05. Its combination of skip connections, BatchNorm, and Dropout ensures robust accuracy and good generalization (Overfit Gap -0.0027 to 0.0025) in deeper structures. The Uni-Neuron Network (Uni-NN) is a solid alternative, performing well in smaller setups (e.g., Test MSE 0.0060, R² 0.8455, Small, LR=0.025), but it doesn’t match AdvUni-NN’s consistency across all groups. Normal networks falter, especially in the Long group (Test MSE 0.0399, R² ≈ -0.0264), while Advanced-Normal peaks at Test MSE 0.0047, R² 0.8792 (Large, LR=0.01) but can’t compete with AdvUni-NN in deeper configurations.

Compute Trade-off: AdvUni-NN’s accuracy comes with longer training times (76–207s) compared to Normal networks (24–43s). Latency stays low (≤0.0005s), but compute cost is a factor for deployment.

Architectural Insight: Uni-NN’s first-layer contribution varies due to ReLU sparsity, while AdvUni-NN stabilizes it with BatchNorm, enhancing overall performance.

Recommendations:

For deployment, use AdvUni-NN with LR=0.05 in Long (313 params) configurations for maximum accuracy and generalization.
For efficiency, opt for Small (165 params) or Mid (277 params) AdvUni-NN, achieving Test MSE ~0.005–0.006.
Future work could test learning rate scheduling, LeakyReLU to address Uni-NN sparsity, or alternative skip connection designs.

----------------------------------------------------------------------------------------------------------------------------

Key Observations and Limitations

1. Limited to Linear Regression Tasks
The experiment targets linear regression (predicting concrete strength), limiting generalizability to non-linear tasks (e.g., classification or polynomial regression). Normal NN’s failure in Long group (R² ≈ -0.0264) suggests poor adaptability to complex patterns. Testing on non-linear datasets (e.g., Boston Housing) could provide broader insights.

2. Uni-NN Sparsity Issues: 
First-layer contributions show weights stuck at zero, hinting at information overload from skip connections. Backpropagation struggles to update weights effectively, reducing Uni-NN’s potential.

3. Parameter Growth in Uni-NN as Size Increases
Uni-NN’s parameter count grows with skip connections: For [8, 6, 6, 1] (165 params), adding a unit to layer 3 ([8, 6, 7, 1]) increases params by 15 (6+1+8). In Large (629 params), training time reaches 42.74s vs. Normal’s 30.41s (633 params) at LR=0.05. This scalability issue raises compute costs in larger models.

4. Other Limitations
Small Dataset: 1030 samples may miss broader complexities (e.g., outliers).
Fixed Hyperparameters: Batch size (64), epochs (10,000), and vanilla gradient descent limit exploration. Normal NN’s poor Long performance (Test MSE 0.0399) suggests more epochs or tuning could help.
No Hyperparameter Tuning: Fixed Dropout rates and layer sizes restrict optimization. AdvUni-NN (Test MSE 0.0044) might improve with tuned Dropout (e.g., 0.2 vs. 0.5).
Optimizer Bias: Vanilla gradient descent may favor AdvUni-NN’s BatchNorm; SGD or momentum could aid Normal NN.

Overall Assessment
The study establishes AdvUni-NN as the best architecture for this regression task (Test MSE 0.0044, R² 0.8859), showcasing the power of skip connections and regularization. However, its linear regression focus, Uni-NN’s sparsity issues, and parameter growth highlight limitations. Future enhancements could include non-linear tasks, alternative activations, and optimized architectures for broader applicability and efficiency.


